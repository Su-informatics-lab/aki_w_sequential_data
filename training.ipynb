{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "550b0bf5-5bd9-4520-a6aa-b62f0d2c2ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional, Union, Dict, Any, Tuple\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "\n",
    "from transformers import PatchTSTConfig, PatchTSTForClassification, Trainer, TrainingArguments\n",
    "\n",
    "from utils import ForecastDFDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34c7783e-a3ba-48ee-b18b-a569a5e57123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "\n",
    "# Import PatchTST classes from Hugging Face Transformers\n",
    "from transformers import PatchTSTConfig, PatchTSTForClassification, Trainer, TrainingArguments\n",
    "\n",
    "def load_and_merge_csvs(data_dir, label_dict, debug=False, max_patients=10):\n",
    "    \"\"\"\n",
    "    Load per-patient CSV files from data_dir.\n",
    "    Each CSV file is read, the patient ID is extracted from the filename,\n",
    "    and the AKI label (from label_dict) is attached to each row.\n",
    "    Returns a merged DataFrame.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "    count = 0\n",
    "    for fname in os.listdir(data_dir):\n",
    "        if fname.endswith(\".csv\"):\n",
    "            csv_path = os.path.join(data_dir, fname)\n",
    "            # Extract patient ID from filename: e.g., \"R94565_combined.csv\" -> \"R94565\"\n",
    "            patient_id = fname.split('_')[0]\n",
    "            df_ts = pd.read_csv(csv_path)\n",
    "            # Create a time index (assumed one row per second)\n",
    "            df_ts[\"time_idx\"] = range(len(df_ts))\n",
    "            # Add patient ID and AKI label (default to 0 if not found)\n",
    "            df_ts[\"ID\"] = patient_id\n",
    "            df_ts[\"Acute_kidney_injury\"] = label_dict.get(patient_id, 0)\n",
    "            all_dfs.append(df_ts)\n",
    "            count += 1\n",
    "            if debug and count >= max_patients:\n",
    "                break\n",
    "    df_merged = pd.concat(all_dfs, ignore_index=True)\n",
    "    return df_merged\n",
    "\n",
    "def truncate_pad_series(df, fixed_length, pad_value=0):\n",
    "    \"\"\"\n",
    "    For one patient's DataFrame df (assumed sorted by time_idx), truncate if length > fixed_length;\n",
    "    if length < fixed_length, pad with pad_value.\n",
    "    Returns a DataFrame with exactly fixed_length rows.\n",
    "    \"\"\"\n",
    "    current_length = len(df)\n",
    "    if current_length >= fixed_length:\n",
    "        return df.iloc[:fixed_length].copy()\n",
    "    else:\n",
    "        pad_df = pd.DataFrame(pad_value, index=range(fixed_length - current_length), columns=df.columns)\n",
    "        # Keep constant columns for 'ID' and 'Acute_kidney_injury'\n",
    "        for col in [\"ID\", \"Acute_kidney_injury\"]:\n",
    "            if col in df.columns:\n",
    "                pad_df[col] = df.iloc[0][col]\n",
    "        # Create a continuing time_idx\n",
    "        pad_df[\"time_idx\"] = range(current_length, fixed_length)\n",
    "        df_out = pd.concat([df, pad_df], ignore_index=True)\n",
    "        return df_out\n",
    "\n",
    "def pool_time_series(df, window_size=60, pooling_method='average'):\n",
    "    \"\"\"\n",
    "    Pool a single patient's time series DataFrame over non-overlapping windows of size `window_size`.\n",
    "    Each window is aggregated per column using the specified pooling method:\n",
    "       'average' -> np.nanmean, 'max' -> np.nanmax, 'median' -> np.nanmedian.\n",
    "    Returns a new DataFrame with ceil(len(df)/window_size) rows.\n",
    "    Non-numeric columns ('ID', 'Acute_kidney_injury', 'time_idx') are preserved.\n",
    "    \"\"\"\n",
    "    exclude_cols = {\"ID\", \"Acute_kidney_injury\", \"time_idx\"}\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols and np.issubdtype(df[col].dtype, np.number)]\n",
    "    pooled_data = []\n",
    "    n = len(df)\n",
    "    num_windows = int(np.ceil(n / window_size))\n",
    "    for i in range(num_windows):\n",
    "        start = i * window_size\n",
    "        end = min((i + 1) * window_size, n)\n",
    "        window = df.iloc[start:end]\n",
    "        pooled_row = {}\n",
    "        pooled_row[\"ID\"] = window.iloc[0][\"ID\"]\n",
    "        pooled_row[\"Acute_kidney_injury\"] = window.iloc[0][\"Acute_kidney_injury\"]\n",
    "        pooled_row[\"time_idx\"] = window[\"time_idx\"].mean()\n",
    "        for col in feature_cols:\n",
    "            if pooling_method == 'average':\n",
    "                pooled_row[col] = np.nanmean(window[col])\n",
    "            elif pooling_method == 'max':\n",
    "                pooled_row[col] = np.nanmax(window[col])\n",
    "            elif pooling_method == 'median':\n",
    "                pooled_row[col] = np.nanmedian(window[col])\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown pooling method: {pooling_method}\")\n",
    "        pooled_data.append(pooled_row)\n",
    "    return pd.DataFrame(pooled_data)\n",
    "\n",
    "# def main(args):\n",
    "#     # Step 1: Load the Excel with AKI labels.\n",
    "#     df_labels = pd.read_excel(\"imputed_demo_data.xlsx\")\n",
    "#     df_labels = df_labels[[\"ID\", \"Acute_kidney_injury\"]].drop_duplicates()\n",
    "#     label_dict = dict(zip(df_labels[\"ID\"], df_labels[\"Acute_kidney_injury\"]))\n",
    "    \n",
    "#     # Step 2: Load per-patient CSVs and merge them.\n",
    "#     merged_df = load_and_merge_csvs(data_dir=args.data_dir, label_dict=label_dict, debug=args.debug, max_patients=args.max_patients)\n",
    "    \n",
    "#     # Step 3: Process each patientâ€™s data to ensure a fixed length of 3 hours.\n",
    "#     # For 1-second resolution, fixed length = 10800; or use pooling to get fewer points.\n",
    "#     processed_dfs = []\n",
    "#     for patient_id, group in merged_df.groupby(\"ID\"):\n",
    "#         group = group.sort_values(\"time_idx\")\n",
    "#         if args.process_mode == \"truncate\":\n",
    "#             processed = truncate_pad_series(group, fixed_length=args.fixed_length, pad_value=0)\n",
    "#         elif args.process_mode == \"pool\":\n",
    "#             processed = pool_time_series(group, window_size=args.pool_window, pooling_method=args.pool_method)\n",
    "#         else:\n",
    "#             processed = group\n",
    "#         processed_dfs.append(processed)\n",
    "#     processed_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    \n",
    "#     # Option: if using pooling, all patients should now have the same number of rows.\n",
    "#     # Split the data by patient ID to get train/val split.\n",
    "#     unique_ids = processed_df[\"ID\"].unique()\n",
    "#     train_ids, val_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)\n",
    "#     train_df = processed_df[processed_df[\"ID\"].isin(train_ids)]\n",
    "#     val_df = processed_df[processed_df[\"ID\"].isin(val_ids)]\n",
    "    \n",
    "#     # Determine the feature columns (time-varying unknowns) for the dataset.\n",
    "#     feature_cols = [col for col in processed_df.columns if col not in {\"ID\", \"Acute_kidney_injury\", \"time_idx\"}]\n",
    "    \n",
    "#     # Create ForecastDFDataset objects.\n",
    "#     # Here, history_length is set to the length of each patient's series (assuming uniform length after processing).\n",
    "#     history_length = train_df.groupby(\"ID\").size().max()\n",
    "#     train_dataset = ForecastDFDataset(\n",
    "#         df=train_df,\n",
    "#         id_col=\"ID\",\n",
    "#         time_col=\"time_idx\",\n",
    "#         target_col=\"Acute_kidney_injury\",\n",
    "#         history_length=history_length,\n",
    "#         forecast_length=1,\n",
    "#         time_varying_unknown_cols=feature_cols,\n",
    "#         static_reals_cols=[],\n",
    "#     )\n",
    "#     val_dataset = ForecastDFDataset(\n",
    "#         df=val_df,\n",
    "#         id_col=\"ID\",\n",
    "#         time_col=\"time_idx\",\n",
    "#         target_col=\"Acute_kidney_injury\",\n",
    "#         history_length=history_length,\n",
    "#         forecast_length=1,\n",
    "#         time_varying_unknown_cols=feature_cols,\n",
    "#         static_reals_cols=[],\n",
    "#     )\n",
    "    \n",
    "#     # For simplicity, we assume the entire dataset fits into memory.\n",
    "#     # Create Hugging Face style datasets (if ForecastDFDataset provides a .to_dataloader() method).\n",
    "#     train_loader = train_dataset.to_dataloader(batch_size=args.batch_size, shuffle=True, mode=\"train\")\n",
    "#     val_loader = val_dataset.to_dataloader(batch_size=args.batch_size, shuffle=False, mode=\"valid\")\n",
    "    \n",
    "#     # Create PatchTST configuration and model for classification.\n",
    "#     config = PatchTSTConfig(\n",
    "#         num_input_channels=len(feature_cols),\n",
    "#         context_length=history_length,  # input sequence length (after processing)\n",
    "#         prediction_length=1,\n",
    "#         num_targets=2,  # binary classification (0 and 1)\n",
    "#         patch_length=args.patch_length,\n",
    "#         patch_stride=args.patch_stride,\n",
    "#         d_model=args.d_model,\n",
    "#         num_hidden_layers=args.num_hidden_layers,\n",
    "#         num_attention_heads=args.num_attention_heads,\n",
    "#     )\n",
    "#     model = PatchTSTForClassification(config)\n",
    "    \n",
    "#     # Setup training arguments\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=args.output_dir,\n",
    "#         evaluation_strategy=\"steps\",\n",
    "#         eval_steps=args.eval_steps,\n",
    "#         logging_steps=args.logging_steps,\n",
    "#         per_device_train_batch_size=args.batch_size,\n",
    "#         per_device_eval_batch_size=args.batch_size,\n",
    "#         num_train_epochs=args.epochs,\n",
    "#         save_steps=args.save_steps,\n",
    "#         save_total_limit=2,\n",
    "#         load_best_model_at_end=True,\n",
    "#         metric_for_best_model=\"loss\",\n",
    "#     )\n",
    "    \n",
    "#     # Define a compute_metrics function for evaluation (using accuracy)\n",
    "#     def compute_metrics(eval_pred):\n",
    "#         logits, labels = eval_pred\n",
    "#         preds = np.argmax(logits, axis=-1)\n",
    "#         accuracy = (preds == labels).mean()\n",
    "#         return {\"accuracy\": accuracy}\n",
    "    \n",
    "#     # Instantiate Trainer\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=train_dataset,\n",
    "#         eval_dataset=val_dataset,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#     )\n",
    "    \n",
    "#     # Train the model and evaluate\n",
    "#     trainer.train()\n",
    "#     trainer.evaluate()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--data_dir\", type=str, default=\"time_series_data_LSTM_10_29_2024\",\n",
    "#                         help=\"Folder with per-patient CSV files.\")\n",
    "#     parser.add_argument(\"--debug\", action=\"store_true\", help=\"Debug mode: load only a few patients.\")\n",
    "#     parser.add_argument(\"--max_patients\", type=int, default=10, help=\"Max patients to load in debug mode.\")\n",
    "#     parser.add_argument(\"--process_mode\", type=str, choices=[\"truncate\", \"pool\", \"none\"], default=\"pool\",\n",
    "#                         help=\"Preprocessing mode: 'truncate' (pad/truncate to fixed length), 'pool' (aggregate over windows), or 'none'.\")\n",
    "#     parser.add_argument(\"--fixed_length\", type=int, default=10800,\n",
    "#                         help=\"Fixed length (number of rows) if using truncate mode (e.g., 10800 for 3 hours at 1 sec resolution).\")\n",
    "#     parser.add_argument(\"--pool_window\", type=int, default=60,\n",
    "#                         help=\"Window size for pooling (e.g., 60 seconds).\")\n",
    "#     parser.add_argument(\"--pool_method\", type=str, choices=[\"average\", \"max\", \"median\"], default=\"average\",\n",
    "#                         help=\"Pooling method if using pool mode.\")\n",
    "#     parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "#     parser.add_argument(\"--patch_length\", type=int, default=16)\n",
    "#     parser.add_argument(\"--patch_stride\", type=int, default=8)\n",
    "#     parser.add_argument(\"--d_model\", type=int, default=64)\n",
    "#     parser.add_argument(\"--num_hidden_layers\", type=int, default=2)\n",
    "#     parser.add_argument(\"--num_attention_heads\", type=int, default=8)\n",
    "#     parser.add_argument(\"--output_dir\", type=str, default=\"./patchtst_checkpoints\")\n",
    "#     parser.add_argument(\"--eval_steps\", type=int, default=100)\n",
    "#     parser.add_argument(\"--logging_steps\", type=int, default=50)\n",
    "#     parser.add_argument(\"--save_steps\", type=int, default=100)\n",
    "#     parser.add_argument(\"--epochs\", type=int, default=10)\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "#     main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fec8792-ca79-4a7d-9f1a-7d6d5b34650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the Excel with AKI labels.\n",
    "df_labels = pd.read_excel(\"imputed_demo_data.xlsx\")\n",
    "df_labels = df_labels[[\"ID\", \"Acute_kidney_injury\"]].drop_duplicates()\n",
    "label_dict = dict(zip(df_labels[\"ID\"], df_labels[\"Acute_kidney_injury\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c5b7400-0a84-4650-b28d-82a98bdef7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load per-patient CSVs and merge them.\n",
    "merged_df = load_and_merge_csvs(data_dir='time_series_data_LSTM_10_29_2024',\n",
    "                                label_dict=label_dict,\n",
    "                                debug=False,\n",
    "                                max_patients=100,\n",
    "                                # max_patients=df_labels.shape[0]\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e55b43b-0b34-43a0-9ea6-148be46887cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/px/d6yfk3wn5xnc9b_s69cbw93m0000gn/T/ipykernel_54167/1251722580.py:82: RuntimeWarning: Mean of empty slice\n",
      "  pooled_row[col] = np.nanmean(window[col])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m processed_dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m patient_id, group \u001b[38;5;129;01min\u001b[39;00m merged_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[43mpool_time_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooling_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maverage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# group = group.sort_values(\"time_idx\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# if args.process_mode == \"truncate\":\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#     processed = truncate_pad_series(group, fixed_length=args.fixed_length, pad_value=0)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#     processed = group\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     processed_dfs\u001b[38;5;241m.\u001b[39mappend(processed)\n",
      "Cell \u001b[0;32mIn[3], line 79\u001b[0m, in \u001b[0;36mpool_time_series\u001b[0;34m(df, window_size, pooling_method)\u001b[0m\n\u001b[1;32m     77\u001b[0m pooled_row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m window\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     78\u001b[0m pooled_row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcute_kidney_injury\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m window\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcute_kidney_injury\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 79\u001b[0m pooled_row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mwindow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m feature_cols:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pooling_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/Downloads/aki_w_sequential_data/.venv/lib/python3.10/site-packages/pandas/core/series.py:6549\u001b[0m, in \u001b[0;36mSeries.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   6541\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   6542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m   6543\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6547\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   6548\u001b[0m ):\n\u001b[0;32m-> 6549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNDFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/aki_w_sequential_data/.venv/lib/python3.10/site-packages/pandas/core/generic.py:12420\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m  12414\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  12415\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  12418\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  12419\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m> 12420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  12421\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m  12422\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/aki_w_sequential_data/.venv/lib/python3.10/site-packages/pandas/core/generic.py:12377\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12373\u001b[0m nv\u001b[38;5;241m.\u001b[39mvalidate_func(name, (), kwargs)\n\u001b[1;32m  12375\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m> 12377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  12378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\n\u001b[1;32m  12379\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/aki_w_sequential_data/.venv/lib/python3.10/site-packages/pandas/core/series.py:6457\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   6452\u001b[0m     \u001b[38;5;66;03m# GH#47500 - change to TypeError to match other methods\u001b[39;00m\n\u001b[1;32m   6453\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   6454\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not allow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwd_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumeric_only\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6455\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-numeric dtypes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6456\u001b[0m     )\n\u001b[0;32m-> 6457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelegate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/aki_w_sequential_data/.venv/lib/python3.10/site-packages/pandas/core/nanops.py:147\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    145\u001b[0m         result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Downloads/aki_w_sequential_data/.venv/lib/python3.10/site-packages/pandas/core/nanops.py:404\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[0;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[0;32m--> 404\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n\u001b[1;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _wrap_results(result, orig_values\u001b[38;5;241m.\u001b[39mdtype, fill_value\u001b[38;5;241m=\u001b[39miNaT)\n",
      "File \u001b[0;32m~/Downloads/aki_w_sequential_data/.venv/lib/python3.10/site-packages/pandas/core/nanops.py:718\u001b[0m, in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    715\u001b[0m     dtype_sum \u001b[38;5;241m=\u001b[39m dtype\n\u001b[1;32m    716\u001b[0m     dtype_count \u001b[38;5;241m=\u001b[39m dtype\n\u001b[0;32m--> 718\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[43m_get_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39msum(axis, dtype\u001b[38;5;241m=\u001b[39mdtype_sum)\n\u001b[1;32m    720\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m _ensure_numeric(the_sum)\n",
      "File \u001b[0;32m~/Downloads/aki_w_sequential_data/.venv/lib/python3.10/site-packages/pandas/core/nanops.py:1486\u001b[0m, in \u001b[0;36m_get_counts\u001b[0;34m(values_shape, mask, axis, dtype)\u001b[0m\n\u001b[1;32m   1484\u001b[0m         n \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m-\u001b[39m mask\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m   1485\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m         n \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mtype(n)\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Downloads/aki_w_sequential_data/.venv/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:3445\u001b[0m, in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   3328\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_prod_dispatcher)\n\u001b[1;32m   3329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprod\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   3330\u001b[0m          initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   3331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3332\u001b[0m \u001b[38;5;124;03m    Return the product of array elements over a given axis.\u001b[39;00m\n\u001b[1;32m   3333\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3443\u001b[0m \u001b[38;5;124;03m    10\u001b[39;00m\n\u001b[1;32m   3444\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprod\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3446\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/aki_w_sequential_data/.venv/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 3: Process each patientâ€™s data to ensure a fixed length of 3 hours.\n",
    "# For 1-second resolution, fixed length = 10800; or use pooling to get fewer points.\n",
    "processed_dfs = []\n",
    "for patient_id, group in merged_df.groupby(\"ID\"):\n",
    "    processed = pool_time_series(group, window_size=60, pooling_method='average')\n",
    "    # group = group.sort_values(\"time_idx\")\n",
    "    # if args.process_mode == \"truncate\":\n",
    "    #     processed = truncate_pad_series(group, fixed_length=args.fixed_length, pad_value=0)\n",
    "    # elif args.process_mode == \"pool\":\n",
    "    #     processed = pool_time_series(group, window_size=args.pool_window, pooling_method=args.pool_method)\n",
    "    # else:\n",
    "    #     processed = group\n",
    "    processed_dfs.append(processed)\n",
    "processed_df = pd.concat(processed_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9f4a11-6b37-41c0-8bae-b6e23f61c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d279055-9d7f-423c-9c56-6e44356052b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: if using pooling, all patients should now have the same number of rows.\n",
    "# Split the data by patient ID to get train/val split.\n",
    "unique_ids = processed_df[\"ID\"].unique()\n",
    "train_ids, val_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)\n",
    "train_df = processed_df[processed_df[\"ID\"].isin(train_ids)]\n",
    "val_df = processed_df[processed_df[\"ID\"].isin(val_ids)]\n",
    "\n",
    "# Determine the feature columns (time-varying unknowns) for the dataset.\n",
    "feature_cols = [col for col in processed_df.columns if col not in {\"ID\", \"Acute_kidney_injury\", \"time_idx\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7415c48-6ee3-4a9a-8ce9-8bc1af30053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ForecastDFDataset objects.\n",
    "# Here, history_length is set to the length of each patient's series (assuming uniform length after processing).\n",
    "history_length = train_df.groupby(\"ID\").size().max()\n",
    "train_dataset = ForecastDFDataset(\n",
    "    df=train_df,\n",
    "    id_col=\"ID\",\n",
    "    time_col=\"time_idx\",\n",
    "    target_col=\"Acute_kidney_injury\",\n",
    "    history_length=history_length,\n",
    "    forecast_length=1,\n",
    "    time_varying_unknown_cols=feature_cols,\n",
    "    static_reals_cols=[],\n",
    ")\n",
    "val_dataset = ForecastDFDataset(\n",
    "    df=val_df,\n",
    "    id_col=\"ID\",\n",
    "    time_col=\"time_idx\",\n",
    "    target_col=\"Acute_kidney_injury\",\n",
    "    history_length=history_length,\n",
    "    forecast_length=1,\n",
    "    time_varying_unknown_cols=feature_cols,\n",
    "    static_reals_cols=[],\n",
    ")\n",
    "\n",
    "# For simplicity, we assume the entire dataset fits into memory.\n",
    "# Create Hugging Face style datasets (if ForecastDFDataset provides a .to_dataloader() method).\n",
    "train_loader = train_dataset.to_dataloader(batch_size=args.batch_size, shuffle=True, mode=\"train\")\n",
    "val_loader = val_dataset.to_dataloader(batch_size=args.batch_size, shuffle=False, mode=\"valid\")\n",
    "\n",
    "# Create PatchTST configuration and model for classification.\n",
    "config = PatchTSTConfig(\n",
    "    num_input_channels=len(feature_cols),\n",
    "    context_length=history_length,  # input sequence length (after processing)\n",
    "    prediction_length=1,\n",
    "    num_targets=2,  # binary classification (0 and 1)\n",
    "    patch_length=args.patch_length,\n",
    "    patch_stride=args.patch_stride,\n",
    "    d_model=args.d_model,\n",
    "    num_hidden_layers=args.num_hidden_layers,\n",
    "    num_attention_heads=args.num_attention_heads,\n",
    ")\n",
    "model = PatchTSTForClassification(config)\n",
    "\n",
    "# Setup training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=args.output_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=args.eval_steps,\n",
    "    logging_steps=args.logging_steps,\n",
    "    per_device_train_batch_size=args.batch_size,\n",
    "    per_device_eval_batch_size=args.batch_size,\n",
    "    num_train_epochs=args.epochs,\n",
    "    save_steps=args.save_steps,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    ")\n",
    "\n",
    "# Define a compute_metrics function for evaluation (using accuracy)\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# Instantiate Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model and evaluate\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccd12bd-4f5f-47d6-825f-1c6b9a36e76a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16698247-3ae4-49c2-9b66-6fc22107f963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42f888b-43ed-4f5d-9952-6e787fd682b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500e211-e1a1-4f22-809c-54995853be4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec61ccba-f903-43c5-8ece-6bf9a1f194f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe3ab90-2b43-47af-9865-fb84ebc77845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c985c872-37de-482b-b621-eda9bc7595b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc7650a-7c5b-49aa-a712-f090d639bb35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464f3f55-da15-445d-8e3f-a682761a4244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1955eb-0f1f-4888-8f5a-966ca8a551e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d046d8cd-9b52-49f1-9d9b-42355e00ee05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b2b4a5-460e-4e1a-9b3e-fdb050ece953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8485a8db-40b2-458b-813e-8f8335dc6007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bb1649-ffea-4d9c-8683-fc9e81504a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ae6ad-599d-4d80-b08c-1a74c808edd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a913c2e-3a28-495d-a2ad-18463d360d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e35a2c-ada4-44db-bd32-7878742e7322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515a0ce1-6da8-4d55-9f38-548802af0006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9f6c42d-5cef-4f4f-a1ba-9e7eb8b52c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_and_merge_csvs(data_dir, label_dict, debug=False, max_patients=10):\n",
    "    \"\"\"\n",
    "    Load per-patient CSV files from data_dir.\n",
    "    Each CSV file is read, the patient ID is extracted from the filename,\n",
    "    and the AKI label (from label_dict) is attached to each row.\n",
    "    Returns a merged DataFrame.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "    count = 0\n",
    "    for fname in os.listdir(data_dir):\n",
    "        if fname.endswith(\".csv\"):\n",
    "            csv_path = os.path.join(data_dir, fname)\n",
    "            # Extract patient ID from filename: e.g., \"R94565_combined.csv\" -> \"R94565\"\n",
    "            patient_id = fname.split('_')[0]\n",
    "            df_ts = pd.read_csv(csv_path)\n",
    "            # Create a time index (assumed one row per second)\n",
    "            df_ts[\"time_idx\"] = range(len(df_ts))\n",
    "            # Add patient ID and AKI label (default to 0 if not found)\n",
    "            df_ts[\"ID\"] = patient_id\n",
    "            df_ts[\"Acute_kidney_injury\"] = label_dict.get(patient_id, 0)\n",
    "            all_dfs.append(df_ts)\n",
    "            count += 1\n",
    "            if debug and count >= max_patients:\n",
    "                break\n",
    "    df_merged = pd.concat(all_dfs, ignore_index=True)\n",
    "    return df_merged\n",
    "\n",
    "def truncate_pad_series(df, fixed_length, pad_value=0):\n",
    "    \"\"\"\n",
    "    For one patient's DataFrame df (assumed sorted by time_idx), truncate if length > fixed_length;\n",
    "    if length < fixed_length, pad with pad_value.\n",
    "    Returns a DataFrame with exactly fixed_length rows.\n",
    "    \"\"\"\n",
    "    current_length = len(df)\n",
    "    if current_length >= fixed_length:\n",
    "        return df.iloc[:fixed_length].copy()\n",
    "    else:\n",
    "        pad_df = pd.DataFrame(pad_value, index=range(fixed_length - current_length), columns=df.columns)\n",
    "        # Keep constant columns for 'ID' and 'Acute_kidney_injury'\n",
    "        for col in [\"ID\", \"Acute_kidney_injury\"]:\n",
    "            if col in df.columns:\n",
    "                pad_df[col] = df.iloc[0][col]\n",
    "        # Create a continuing time_idx\n",
    "        pad_df[\"time_idx\"] = range(current_length, fixed_length)\n",
    "        df_out = pd.concat([df, pad_df], ignore_index=True)\n",
    "        return df_out\n",
    "\n",
    "def pool_time_series(df, window_size=60, pooling_method='average'):\n",
    "    \"\"\"\n",
    "    Pool a single patient's time series DataFrame over non-overlapping windows of size `window_size`.\n",
    "    Each window is aggregated per column using the specified pooling method:\n",
    "       'average' -> np.nanmean, 'max' -> np.nanmax, 'median' -> np.nanmedian.\n",
    "    Returns a new DataFrame with ceil(len(df)/window_size) rows.\n",
    "    Non-numeric columns ('ID', 'Acute_kidney_injury', 'time_idx') are preserved.\n",
    "    \"\"\"\n",
    "    exclude_cols = {\"ID\", \"Acute_kidney_injury\", \"time_idx\"}\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols and np.issubdtype(df[col].dtype, np.number)]\n",
    "    pooled_data = []\n",
    "    n = len(df)\n",
    "    num_windows = int(np.ceil(n / window_size))\n",
    "    for i in range(num_windows):\n",
    "        start = i * window_size\n",
    "        end = min((i + 1) * window_size, n)\n",
    "        window = df.iloc[start:end]\n",
    "        pooled_row = {}\n",
    "        pooled_row[\"ID\"] = window.iloc[0][\"ID\"]\n",
    "        pooled_row[\"Acute_kidney_injury\"] = window.iloc[0][\"Acute_kidney_injury\"]\n",
    "        pooled_row[\"time_idx\"] = window[\"time_idx\"].mean()\n",
    "        for col in feature_cols:\n",
    "            if pooling_method == 'average':\n",
    "                pooled_row[col] = np.nanmean(window[col])\n",
    "            elif pooling_method == 'max':\n",
    "                pooled_row[col] = np.nanmax(window[col])\n",
    "            elif pooling_method == 'median':\n",
    "                pooled_row[col] = np.nanmedian(window[col])\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown pooling method: {pooling_method}\")\n",
    "        pooled_data.append(pooled_row)\n",
    "    return pd.DataFrame(pooled_data)\n",
    "\n",
    "def main(args):\n",
    "    # Step 1: Load the Excel with AKI labels.\n",
    "    df_labels = pd.read_excel(\"imputed_demo_data.xlsx\")\n",
    "    df_labels = df_labels[[\"ID\", \"Acute_kidney_injury\"]].drop_duplicates()\n",
    "    label_dict = dict(zip(df_labels[\"ID\"], df_labels[\"Acute_kidney_injury\"]))\n",
    "    \n",
    "    # Step 2: Load per-patient CSVs and merge them.\n",
    "    merged_df = load_and_merge_csvs(data_dir=args.data_dir, label_dict=label_dict, debug=args.debug, max_patients=args.max_patients)\n",
    "    \n",
    "    # Step 3: Process each patientâ€™s data to ensure a fixed length of 3 hours.\n",
    "    # For 1-second resolution, fixed length = 10800; or use pooling to get fewer points.\n",
    "    processed_dfs = []\n",
    "    for patient_id, group in merged_df.groupby(\"ID\"):\n",
    "        group = group.sort_values(\"time_idx\")\n",
    "        if args.process_mode == \"truncate\":\n",
    "            processed = truncate_pad_series(group, fixed_length=args.fixed_length, pad_value=0)\n",
    "        elif args.process_mode == \"pool\":\n",
    "            processed = pool_time_series(group, window_size=args.pool_window, pooling_method=args.pool_method)\n",
    "        else:\n",
    "            processed = group\n",
    "        processed_dfs.append(processed)\n",
    "    processed_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    \n",
    "    # Option: if using pooling, all patients should now have the same number of rows.\n",
    "    # Split the data by patient ID to get train/val split.\n",
    "    unique_ids = processed_df[\"ID\"].unique()\n",
    "    train_ids, val_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)\n",
    "    train_df = processed_df[processed_df[\"ID\"].isin(train_ids)]\n",
    "    val_df = processed_df[processed_df[\"ID\"].isin(val_ids)]\n",
    "    \n",
    "    # Determine the feature columns (time-varying unknowns) for the dataset.\n",
    "    feature_cols = [col for col in processed_df.columns if col not in {\"ID\", \"Acute_kidney_injury\", \"time_idx\"}]\n",
    "    \n",
    "    # Create ForecastDFDataset objects.\n",
    "    # Here, history_length is set to the length of each patient's series (assuming uniform length after processing).\n",
    "    history_length = train_df.groupby(\"ID\").size().max()\n",
    "    train_dataset = ForecastDFDataset(\n",
    "        df=train_df,\n",
    "        id_col=\"ID\",\n",
    "        time_col=\"time_idx\",\n",
    "        target_col=\"Acute_kidney_injury\",\n",
    "        history_length=history_length,\n",
    "        forecast_length=1,\n",
    "        time_varying_unknown_cols=feature_cols,\n",
    "        static_reals_cols=[],\n",
    "    )\n",
    "    val_dataset = ForecastDFDataset(\n",
    "        df=val_df,\n",
    "        id_col=\"ID\",\n",
    "        time_col=\"time_idx\",\n",
    "        target_col=\"Acute_kidney_injury\",\n",
    "        history_length=history_length,\n",
    "        forecast_length=1,\n",
    "        time_varying_unknown_cols=feature_cols,\n",
    "        static_reals_cols=[],\n",
    "    )\n",
    "    \n",
    "    # For simplicity, we assume the entire dataset fits into memory.\n",
    "    # Create Hugging Face style datasets (if ForecastDFDataset provides a .to_dataloader() method).\n",
    "    train_loader = train_dataset.to_dataloader(batch_size=args.batch_size, shuffle=True, mode=\"train\")\n",
    "    val_loader = val_dataset.to_dataloader(batch_size=args.batch_size, shuffle=False, mode=\"valid\")\n",
    "    \n",
    "    # Create PatchTST configuration and model for classification.\n",
    "    config = PatchTSTConfig(\n",
    "        num_input_channels=len(feature_cols),\n",
    "        context_length=history_length,  # input sequence length (after processing)\n",
    "        prediction_length=1,\n",
    "        num_targets=2,  # binary classification (0 and 1)\n",
    "        patch_length=args.patch_length,\n",
    "        patch_stride=args.patch_stride,\n",
    "        d_model=args.d_model,\n",
    "        num_hidden_layers=args.num_hidden_layers,\n",
    "        num_attention_heads=args.num_attention_heads,\n",
    "    )\n",
    "    model = PatchTSTForClassification(config)\n",
    "    \n",
    "    # Setup training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=args.eval_steps,\n",
    "        logging_steps=args.logging_steps,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        num_train_epochs=args.epochs,\n",
    "        save_steps=args.save_steps,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "    )\n",
    "    \n",
    "    # Define a compute_metrics function for evaluation (using accuracy)\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        accuracy = (preds == labels).mean()\n",
    "        return {\"accuracy\": accuracy}\n",
    "    \n",
    "    # Instantiate Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # Train the model and evaluate\n",
    "    trainer.train()\n",
    "    trainer.evaluate()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--data_dir\", type=str, default=\"time_series_data_LSTM_10_29_2024\",\n",
    "#                         help=\"Folder with per-patient CSV files.\")\n",
    "#     parser.add_argument(\"--debug\", action=\"store_true\", help=\"Debug mode: load only a few patients.\")\n",
    "#     parser.add_argument(\"--max_patients\", type=int, default=10, help=\"Max patients to load in debug mode.\")\n",
    "#     parser.add_argument(\"--process_mode\", type=str, choices=[\"truncate\", \"pool\", \"none\"], default=\"pool\",\n",
    "#                         help=\"Preprocessing mode: 'truncate' (pad/truncate to fixed length), 'pool' (aggregate over windows), or 'none'.\")\n",
    "#     parser.add_argument(\"--fixed_length\", type=int, default=10800,\n",
    "#                         help=\"Fixed length (number of rows) if using truncate mode (e.g., 10800 for 3 hours at 1 sec resolution).\")\n",
    "#     parser.add_argument(\"--pool_window\", type=int, default=60,\n",
    "#                         help=\"Window size for pooling (e.g., 60 seconds).\")\n",
    "#     parser.add_argument(\"--pool_method\", type=str, choices=[\"average\", \"max\", \"median\"], default=\"average\",\n",
    "#                         help=\"Pooling method if using pool mode.\")\n",
    "#     parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "#     parser.add_argument(\"--patch_length\", type=int, default=16)\n",
    "#     parser.add_argument(\"--patch_stride\", type=int, default=8)\n",
    "#     parser.add_argument(\"--d_model\", type=int, default=64)\n",
    "#     parser.add_argument(\"--num_hidden_layers\", type=int, default=2)\n",
    "#     parser.add_argument(\"--num_attention_heads\", type=int, default=8)\n",
    "#     parser.add_argument(\"--output_dir\", type=str, default=\"./patchtst_checkpoints\")\n",
    "#     parser.add_argument(\"--eval_steps\", type=int, default=100)\n",
    "#     parser.add_argument(\"--logging_steps\", type=int, default=50)\n",
    "#     parser.add_argument(\"--save_steps\", type=int, default=100)\n",
    "#     parser.add_argument(\"--epochs\", type=int, default=10)\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "#     main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2842c93d-bfe6-447e-919f-2f917a0976c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the Excel with AKI labels.\n",
    "df_labels = pd.read_excel(\"imputed_demo_data.xlsx\")\n",
    "df_labels = df_labels[[\"ID\", \"Acute_kidney_injury\"]].drop_duplicates()\n",
    "label_dict = dict(zip(df_labels[\"ID\"], df_labels[\"Acute_kidney_injury\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dabed77-a7d7-425b-bb33-255f3f7ddb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load per-patient CSVs and merge them\n",
    "merged_df = load_and_merge_csvs(data_dir='time_series_data_LSTM_10_29_2024', \n",
    "                                label_dict=label_dict, \n",
    "                                debug=False, \n",
    "                                max_patients=df_labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fc7da4-93ab-4242-b20a-879703095029",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# 1. Pooling over 60-second windows.\n",
    "# For a 3-hour period, 3*3600/60 = 180 windows.\n",
    "# pool_method = mean\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c0f3f6f-18de-4cee-9df8-c3b013d5ef14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TVexp</th>\n",
       "      <th>MVexp</th>\n",
       "      <th>RRtotal</th>\n",
       "      <th>Circuit_O2</th>\n",
       "      <th>Pmean</th>\n",
       "      <th>TVinsp</th>\n",
       "      <th>MVinsp</th>\n",
       "      <th>PEEPe_i</th>\n",
       "      <th>CO</th>\n",
       "      <th>CI</th>\n",
       "      <th>...</th>\n",
       "      <th>rSO2_Ch1</th>\n",
       "      <th>rSO2_Ch2</th>\n",
       "      <th>rSO2_Ch3</th>\n",
       "      <th>SpO2</th>\n",
       "      <th>ET_CO2</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>FiO2</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>ID</th>\n",
       "      <th>Acute_kidney_injury</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>R94565</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>R94565</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>R94565</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>R94565</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>R94565</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27881047</th>\n",
       "      <td>316.0</td>\n",
       "      <td>3.88</td>\n",
       "      <td>12.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>4.18</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.36</td>\n",
       "      <td>3.22</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81.0</td>\n",
       "      <td>13240</td>\n",
       "      <td>S25956</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27881048</th>\n",
       "      <td>316.0</td>\n",
       "      <td>3.88</td>\n",
       "      <td>12.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>4.18</td>\n",
       "      <td>2.7</td>\n",
       "      <td>6.15</td>\n",
       "      <td>3.70</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81.0</td>\n",
       "      <td>13241</td>\n",
       "      <td>S25956</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27881049</th>\n",
       "      <td>316.0</td>\n",
       "      <td>3.88</td>\n",
       "      <td>12.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>4.18</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.92</td>\n",
       "      <td>3.56</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81.0</td>\n",
       "      <td>13242</td>\n",
       "      <td>S25956</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27881050</th>\n",
       "      <td>316.0</td>\n",
       "      <td>3.88</td>\n",
       "      <td>12.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>4.18</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.96</td>\n",
       "      <td>3.59</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81.0</td>\n",
       "      <td>13243</td>\n",
       "      <td>S25956</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27881051</th>\n",
       "      <td>332.0</td>\n",
       "      <td>3.88</td>\n",
       "      <td>12.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>4.18</td>\n",
       "      <td>1.8</td>\n",
       "      <td>6.00</td>\n",
       "      <td>3.61</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.0</td>\n",
       "      <td>13244</td>\n",
       "      <td>S25956</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27881052 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          TVexp  MVexp  RRtotal  Circuit_O2  Pmean  TVinsp  MVinsp  PEEPe_i  \\\n",
       "0           NaN    NaN      NaN         NaN    NaN     NaN     NaN      NaN   \n",
       "1           NaN    NaN      NaN         NaN    NaN     NaN     NaN      NaN   \n",
       "2           NaN    NaN      NaN         NaN    NaN     NaN     NaN      NaN   \n",
       "3           NaN    NaN      NaN         NaN    NaN     NaN     NaN      NaN   \n",
       "4           NaN    NaN      NaN         NaN    NaN     NaN     NaN      NaN   \n",
       "...         ...    ...      ...         ...    ...     ...     ...      ...   \n",
       "27881047  316.0   3.88     12.0        81.0    5.0   348.0    4.18      2.7   \n",
       "27881048  316.0   3.88     12.0        81.0    5.0   348.0    4.18      2.7   \n",
       "27881049  316.0   3.88     12.0        81.0    5.0   348.0    4.18      2.7   \n",
       "27881050  316.0   3.88     12.0        81.0    5.0   348.0    4.18      2.7   \n",
       "27881051  332.0   3.88     12.0        82.0    5.0   346.0    4.18      1.8   \n",
       "\n",
       "            CO    CI  ...  rSO2_Ch1  rSO2_Ch2  rSO2_Ch3   SpO2  ET_CO2  TEMP  \\\n",
       "0          NaN   NaN  ...       NaN       NaN       NaN  100.0     0.0   NaN   \n",
       "1          NaN   NaN  ...       NaN       NaN       NaN  100.0     0.0   NaN   \n",
       "2          NaN   NaN  ...       NaN       NaN       NaN  100.0     0.0   NaN   \n",
       "3          NaN   NaN  ...       NaN       NaN       NaN  100.0     0.0   NaN   \n",
       "4          NaN   NaN  ...       NaN       NaN       NaN  100.0     0.0   NaN   \n",
       "...        ...   ...  ...       ...       ...       ...    ...     ...   ...   \n",
       "27881047  5.36  3.22  ...      71.0      68.0      80.0  100.0     NaN   NaN   \n",
       "27881048  6.15  3.70  ...      71.0      68.0      80.0  100.0     NaN   NaN   \n",
       "27881049  5.92  3.56  ...      71.0      68.0      80.0  100.0     NaN   NaN   \n",
       "27881050  5.96  3.59  ...      71.0      68.0      80.0  100.0     NaN   NaN   \n",
       "27881051  6.00  3.61  ...      71.0      68.0      80.0  100.0     NaN   NaN   \n",
       "\n",
       "          FiO2  time_idx      ID  Acute_kidney_injury  \n",
       "0          NaN         0  R94565                  0.0  \n",
       "1          NaN         1  R94565                  0.0  \n",
       "2          NaN         2  R94565                  0.0  \n",
       "3          NaN         3  R94565                  0.0  \n",
       "4          NaN         4  R94565                  0.0  \n",
       "...        ...       ...     ...                  ...  \n",
       "27881047  81.0     13240  S25956                  0.0  \n",
       "27881048  81.0     13241  S25956                  0.0  \n",
       "27881049  81.0     13242  S25956                  0.0  \n",
       "27881050  81.0     13243  S25956                  0.0  \n",
       "27881051  82.0     13244  S25956                  0.0  \n",
       "\n",
       "[27881052 rows x 29 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd2b5aa4-ba9f-4b69-af43-67ba81bec2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/px/d6yfk3wn5xnc9b_s69cbw93m0000gn/T/ipykernel_53466/3107559268.py:585: RuntimeWarning: Mean of empty slice\n",
      "  pooled_row[col] = np.nanmean(window[col])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/Downloads/aki_w_sequential_data/.venv/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   6310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6311\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6312\u001b[0;31m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6313\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute '_cacher'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/px/d6yfk3wn5xnc9b_s69cbw93m0000gn/T/ipykernel_53466/2663527079.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#     processed = pool_time_series(group_fixed, window_size=args.pool_window, pooling_method=args.pool_method)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#     processed = group_fixed  # default to fixed-length series without further pooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprocess_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pool'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool_time_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_fixed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooling_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'average'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mprocessed_dfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprocessed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_dfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/px/d6yfk3wn5xnc9b_s69cbw93m0000gn/T/ipykernel_53466/3107559268.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(df, window_size, pooling_method)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0mpooled_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Acute_kidney_injury\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Acute_kidney_injury\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mpooled_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time_idx\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time_idx\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpooling_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'average'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m                 \u001b[0mpooled_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpooling_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'max'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m                 \u001b[0mpooled_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpooling_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'median'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/aki_w_sequential_data/.venv/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4074\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4075\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4076\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4077\u001b[0m             ):\n\u001b[0;32m-> 4078\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4080\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mis_mi\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4081\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/aki_w_sequential_data/.venv/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   4635\u001b[0m             \u001b[0;31m# All places that call _get_item_cache have unique columns,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4636\u001b[0m             \u001b[0;31m#  pending resolution of GH#33047\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4638\u001b[0m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4639\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4641\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/aki_w_sequential_data/.venv/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   4010\u001b[0m             \u001b[0mcol_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4011\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_col_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_mgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4013\u001b[0m             \u001b[0;31m# this is a cached value, mark it so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4014\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_as_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4015\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/aki_w_sequential_data/.venv/lib/python3.10/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, item, cacher)\u001b[0m\n\u001b[1;32m   1474\u001b[0m         \u001b[0mcacher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \"\"\"\n\u001b[1;32m   1476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1478\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cacher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcacher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Downloads/aki_w_sequential_data/.venv/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   6308\u001b[0m         \u001b[0;31m# e.g. ``obj.x`` and ``obj.x = 4`` will always reference/modify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6309\u001b[0m         \u001b[0;31m# the same attribute.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6311\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6312\u001b[0;31m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6313\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6314\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6315\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 3: Process each patientâ€™s data to ensure a fixed length of 3 hours,\n",
    "# then apply pooling if requested.\n",
    "processed_dfs = []\n",
    "for patient_id, group in merged_df.groupby(\"ID\"):\n",
    "    group = group.sort_values(\"time_idx\")\n",
    "    # First, truncate/pad to fixed length (3 hours = 10800 seconds)\n",
    "    group_fixed = truncate_pad_series(group, fixed_length=10800, pad_value=0)\n",
    "    # if args.process_mode == \"truncate\":\n",
    "    #     processed = group_fixed\n",
    "    # elif args.process_mode == \"pool\":\n",
    "    #     # Now pool over non-overlapping windows (e.g., window_size=60 seconds)\n",
    "    #     processed = pool_time_series(group_fixed, window_size=args.pool_window, pooling_method=args.pool_method)\n",
    "    # else:\n",
    "    #     processed = group_fixed  # default to fixed-length series without further pooling\n",
    "    process_mode = 'pool'\n",
    "    processed = pool_time_series(group_fixed, window_size=60, pooling_method='average')\n",
    "    processed_dfs.append(processed)\n",
    "processed_df = pd.concat(processed_dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7260b857-6243-4dfb-b10f-cb4b374ab391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: if using pooling, all patients should now have the same number of rows.\n",
    "# Split the data by patient ID to get train/val split.\n",
    "unique_ids = processed_df[\"ID\"].unique()\n",
    "train_ids, val_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)\n",
    "train_df = processed_df[processed_df[\"ID\"].isin(train_ids)]\n",
    "val_df = processed_df[processed_df[\"ID\"].isin(val_ids)]\n",
    "\n",
    "# Determine the feature columns (time-varying unknowns) for the dataset.\n",
    "feature_cols = [col for col in processed_df.columns if col not in {\"ID\", \"Acute_kidney_injury\", \"time_idx\"}]\n",
    "\n",
    "# Create ForecastDFDataset objects.\n",
    "# Here, history_length is set to the length of each patient's series (assuming uniform length after processing).\n",
    "history_length = train_df.groupby(\"ID\").size().max()\n",
    "train_dataset = ForecastDFDataset(\n",
    "    df=train_df,\n",
    "    id_col=\"ID\",\n",
    "    time_col=\"time_idx\",\n",
    "    target_col=\"Acute_kidney_injury\",\n",
    "    history_length=history_length,\n",
    "    forecast_length=1,\n",
    "    time_varying_unknown_cols=feature_cols,\n",
    "    static_reals_cols=[],\n",
    ")\n",
    "val_dataset = ForecastDFDataset(\n",
    "    df=val_df,\n",
    "    id_col=\"ID\",\n",
    "    time_col=\"time_idx\",\n",
    "    target_col=\"Acute_kidney_injury\",\n",
    "    history_length=history_length,\n",
    "    forecast_length=1,\n",
    "    time_varying_unknown_cols=feature_cols,\n",
    "    static_reals_cols=[],\n",
    ")\n",
    "\n",
    "# For simplicity, we assume the entire dataset fits into memory.\n",
    "# Create Hugging Face style datasets (if ForecastDFDataset provides a .to_dataloader() method).\n",
    "train_loader = train_dataset.to_dataloader(batch_size=args.batch_size, shuffle=True, mode=\"train\")\n",
    "val_loader = val_dataset.to_dataloader(batch_size=args.batch_size, shuffle=False, mode=\"valid\")\n",
    "\n",
    "# Create PatchTST configuration and model for classification.\n",
    "config = PatchTSTConfig(\n",
    "    num_input_channels=len(feature_cols),\n",
    "    context_length=history_length,  # input sequence length (after processing)\n",
    "    prediction_length=1,\n",
    "    num_targets=2,  # binary classification (0 and 1)\n",
    "    patch_length=args.patch_length,\n",
    "    patch_stride=args.patch_stride,\n",
    "    d_model=args.d_model,\n",
    "    num_hidden_layers=args.num_hidden_layers,\n",
    "    num_attention_heads=args.num_attention_heads,\n",
    ")\n",
    "model = PatchTSTForClassification(config)\n",
    "\n",
    "# Setup training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=args.output_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=args.eval_steps,\n",
    "    logging_steps=args.logging_steps,\n",
    "    per_device_train_batch_size=args.batch_size,\n",
    "    per_device_eval_batch_size=args.batch_size,\n",
    "    num_train_epochs=args.epochs,\n",
    "    save_steps=args.save_steps,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    ")\n",
    "\n",
    "# Define a compute_metrics function for evaluation (using accuracy)\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# Instantiate Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model and evaluate\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
